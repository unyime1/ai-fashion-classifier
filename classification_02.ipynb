{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJT8SINV7Qhi0Y88/iVThz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unyime1/ai-fashion-classifier/blob/main/classification_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRCQJ9bJn-Vq"
      },
      "source": [
        "A deep neural network to classify items in the MINST fashion dataset\n",
        "https://github.com/zalandoresearch/fashion-mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbtR__6nb00v"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6qFGcuBjvIC",
        "outputId": "fbaa37c8-938a-43e9-c76b-0377077d2c6c"
      },
      "source": [
        "# split datasets to train and test sets\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-9sX2f-pvRy"
      },
      "source": [
        "# list of classification names for indexing\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igfewa6alrwd",
        "outputId": "817b7dad-708c-4c0d-c9fe-753a065c6e56"
      },
      "source": [
        "# show a training sample\n",
        "print(f\"Training sample:\\n{train_data[0]}\\n\")\n",
        "print(f\"Training label:\\n{train_labels[0]}\\n\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sample:\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
            "    0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
            "   54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
            "  144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
            "  107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
            "  216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
            "  223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
            "  235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
            "  180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
            "  169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
            "  198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
            "  232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
            "  222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            "  211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
            "  224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
            "  255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
            "  188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
            "  168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
            "  239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
            "  199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
            "  195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
            "  210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
            "  182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "\n",
            "Training label:\n",
            "9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2aoqInZmzqA",
        "outputId": "9f55ec7f-6ebe-4480-c17f-65bfa94208cc"
      },
      "source": [
        "# shape of a training example\n",
        "train_data[0].shape, train_labels[0].shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((28, 28), ())"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "QCAnos7upyYs",
        "outputId": "1d14b512-8a2e-479c-e513-f483228aa9ce"
      },
      "source": [
        "# visualize training sample\n",
        "index = 9000\n",
        "plt.imshow(train_data[index], cmap=plt.cm.binary);\n",
        "plt.title(class_names[train_labels[index]])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Dress')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIVaMJDeq8VJ"
      },
      "source": [
        "# show random images of training sample\n",
        "def get_random_data():\n",
        "  plt.figure(figsize=(7,7))\n",
        "  for i in range(4):\n",
        "    ax = plt.subplot(2, 2, i+1)\n",
        "    rand_index = random.choice(range(len(train_data)))\n",
        "    plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n",
        "    plt.title(class_names[train_labels[rand_index]])\n",
        "    plt.axis(False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "tXsHdEjYsktm",
        "outputId": "e4c24eca-9c6c-45df-fed4-c2db6f894625"
      },
      "source": [
        "get_random_data()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeO_jiaSAiwg"
      },
      "source": [
        "## Build a neural network classification model with:\n",
        "1. Input shape = 28 x 28 (the shape of MINST sample image)\n",
        "2. Output shape = 10 (corresponds to one per class of clothing)\n",
        "3. Loss Function = tf.keras.losses.CategoricalCrossentropy() if labels is onehot encoded else use SparseCategoricalCrossentropy()\n",
        "4. Output layer activation = softmax(ideal for multiclassification instead of sigmoid which is ideal for binary classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV0guDLjBYLs",
        "outputId": "630f145c-8ec2-47f6-d27b-6415172a9c47"
      },
      "source": [
        "# set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# create a model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# add layers\n",
        "model.add(tf.keras.layers.Flatten(\n",
        "    input_shape=(28,28))),  # flatten data to linear vector\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=4, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=4, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=10, activation=tf.keras.activations.softmax))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# train model on training data\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    tf.one_hot(train_labels, depth=10),\n",
        "    epochs=100,\n",
        "    validation_data=(test_data, tf.one_hot(test_labels, depth=10))\n",
        ")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 9s 3ms/step - loss: 2.1772 - accuracy: 0.1593 - val_loss: 1.8122 - val_accuracy: 0.2049\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.7162 - accuracy: 0.2447 - val_loss: 1.6517 - val_accuracy: 0.2895\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.6362 - accuracy: 0.2834 - val_loss: 1.6411 - val_accuracy: 0.3092\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.6066 - accuracy: 0.2948 - val_loss: 1.6071 - val_accuracy: 0.2957\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.6069 - accuracy: 0.3020 - val_loss: 1.5915 - val_accuracy: 0.2866\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5841 - accuracy: 0.3118 - val_loss: 1.5669 - val_accuracy: 0.3257\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5818 - accuracy: 0.3174 - val_loss: 1.5601 - val_accuracy: 0.3289\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5784 - accuracy: 0.3148 - val_loss: 1.5766 - val_accuracy: 0.3262\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5713 - accuracy: 0.3250 - val_loss: 1.5652 - val_accuracy: 0.3138\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5689 - accuracy: 0.3201 - val_loss: 1.5680 - val_accuracy: 0.3045\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5637 - accuracy: 0.3223 - val_loss: 1.5795 - val_accuracy: 0.3194\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5631 - accuracy: 0.3286 - val_loss: 1.5501 - val_accuracy: 0.3307\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5640 - accuracy: 0.3229 - val_loss: 1.5558 - val_accuracy: 0.3357\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5642 - accuracy: 0.3268 - val_loss: 1.5565 - val_accuracy: 0.3354\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5685 - accuracy: 0.3268 - val_loss: 1.5605 - val_accuracy: 0.3370\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5654 - accuracy: 0.3307 - val_loss: 1.5756 - val_accuracy: 0.3322\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5680 - accuracy: 0.3268 - val_loss: 1.5527 - val_accuracy: 0.3344\n",
            "Epoch 18/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5615 - accuracy: 0.3271 - val_loss: 1.5647 - val_accuracy: 0.3309\n",
            "Epoch 19/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5585 - accuracy: 0.3286 - val_loss: 1.5507 - val_accuracy: 0.3334\n",
            "Epoch 20/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5600 - accuracy: 0.3260 - val_loss: 1.5574 - val_accuracy: 0.3355\n",
            "Epoch 21/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5552 - accuracy: 0.3331 - val_loss: 1.5510 - val_accuracy: 0.3366\n",
            "Epoch 22/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5545 - accuracy: 0.3280 - val_loss: 1.5420 - val_accuracy: 0.3421\n",
            "Epoch 23/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5533 - accuracy: 0.3334 - val_loss: 1.5693 - val_accuracy: 0.3284\n",
            "Epoch 24/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5611 - accuracy: 0.3307 - val_loss: 1.5617 - val_accuracy: 0.3385\n",
            "Epoch 25/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5682 - accuracy: 0.3357 - val_loss: 1.5647 - val_accuracy: 0.3384\n",
            "Epoch 26/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5669 - accuracy: 0.3338 - val_loss: 1.5592 - val_accuracy: 0.3347\n",
            "Epoch 27/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5509 - accuracy: 0.3329 - val_loss: 1.5782 - val_accuracy: 0.2978\n",
            "Epoch 28/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5508 - accuracy: 0.3297 - val_loss: 1.5309 - val_accuracy: 0.3439\n",
            "Epoch 29/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5472 - accuracy: 0.3341 - val_loss: 1.5417 - val_accuracy: 0.3394\n",
            "Epoch 30/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5548 - accuracy: 0.3311 - val_loss: 1.5579 - val_accuracy: 0.3364\n",
            "Epoch 31/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5434 - accuracy: 0.3358 - val_loss: 1.5451 - val_accuracy: 0.3301\n",
            "Epoch 32/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5460 - accuracy: 0.3402 - val_loss: 1.5338 - val_accuracy: 0.3449\n",
            "Epoch 33/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5466 - accuracy: 0.3382 - val_loss: 1.5323 - val_accuracy: 0.3449\n",
            "Epoch 34/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5722 - accuracy: 0.3319 - val_loss: 1.5679 - val_accuracy: 0.3300\n",
            "Epoch 35/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5443 - accuracy: 0.3362 - val_loss: 1.5516 - val_accuracy: 0.3424\n",
            "Epoch 36/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5452 - accuracy: 0.3393 - val_loss: 1.5464 - val_accuracy: 0.3440\n",
            "Epoch 37/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5475 - accuracy: 0.3371 - val_loss: 1.5394 - val_accuracy: 0.3422\n",
            "Epoch 38/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5486 - accuracy: 0.3359 - val_loss: 1.5418 - val_accuracy: 0.3464\n",
            "Epoch 39/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5491 - accuracy: 0.3386 - val_loss: 1.5666 - val_accuracy: 0.3336\n",
            "Epoch 40/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5553 - accuracy: 0.3377 - val_loss: 1.5629 - val_accuracy: 0.3402\n",
            "Epoch 41/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5621 - accuracy: 0.3341 - val_loss: 1.5808 - val_accuracy: 0.3262\n",
            "Epoch 42/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5616 - accuracy: 0.3302 - val_loss: 1.5562 - val_accuracy: 0.3414\n",
            "Epoch 43/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5525 - accuracy: 0.3362 - val_loss: 1.5424 - val_accuracy: 0.3454\n",
            "Epoch 44/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5514 - accuracy: 0.3416 - val_loss: 1.5433 - val_accuracy: 0.3459\n",
            "Epoch 45/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5453 - accuracy: 0.3396 - val_loss: 1.5372 - val_accuracy: 0.3445\n",
            "Epoch 46/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5430 - accuracy: 0.3384 - val_loss: 1.5365 - val_accuracy: 0.3446\n",
            "Epoch 47/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5391 - accuracy: 0.3386 - val_loss: 1.5359 - val_accuracy: 0.3443\n",
            "Epoch 48/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5497 - accuracy: 0.3385 - val_loss: 1.5748 - val_accuracy: 0.3390\n",
            "Epoch 49/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5607 - accuracy: 0.3381 - val_loss: 1.5432 - val_accuracy: 0.3464\n",
            "Epoch 50/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5492 - accuracy: 0.3392 - val_loss: 1.5489 - val_accuracy: 0.3430\n",
            "Epoch 51/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5509 - accuracy: 0.3397 - val_loss: 1.5698 - val_accuracy: 0.3378\n",
            "Epoch 52/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5552 - accuracy: 0.3361 - val_loss: 1.5414 - val_accuracy: 0.3434\n",
            "Epoch 53/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5401 - accuracy: 0.3377 - val_loss: 1.5410 - val_accuracy: 0.3449\n",
            "Epoch 54/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5402 - accuracy: 0.3401 - val_loss: 1.5376 - val_accuracy: 0.3432\n",
            "Epoch 55/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5390 - accuracy: 0.3403 - val_loss: 1.5289 - val_accuracy: 0.3465\n",
            "Epoch 56/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5377 - accuracy: 0.3413 - val_loss: 1.5448 - val_accuracy: 0.3380\n",
            "Epoch 57/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5445 - accuracy: 0.3379 - val_loss: 1.5446 - val_accuracy: 0.3353\n",
            "Epoch 58/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5375 - accuracy: 0.3387 - val_loss: 1.5336 - val_accuracy: 0.3434\n",
            "Epoch 59/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5494 - accuracy: 0.3406 - val_loss: 1.5513 - val_accuracy: 0.3380\n",
            "Epoch 60/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5432 - accuracy: 0.3384 - val_loss: 1.5459 - val_accuracy: 0.3410\n",
            "Epoch 61/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5391 - accuracy: 0.3408 - val_loss: 1.5464 - val_accuracy: 0.3387\n",
            "Epoch 62/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5374 - accuracy: 0.3416 - val_loss: 1.5382 - val_accuracy: 0.3439\n",
            "Epoch 63/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5483 - accuracy: 0.3416 - val_loss: 1.5687 - val_accuracy: 0.3391\n",
            "Epoch 64/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5465 - accuracy: 0.3404 - val_loss: 1.5414 - val_accuracy: 0.3446\n",
            "Epoch 65/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5440 - accuracy: 0.3418 - val_loss: 1.5478 - val_accuracy: 0.3434\n",
            "Epoch 66/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5453 - accuracy: 0.3413 - val_loss: 1.5531 - val_accuracy: 0.3442\n",
            "Epoch 67/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5508 - accuracy: 0.3416 - val_loss: 1.5557 - val_accuracy: 0.3405\n",
            "Epoch 68/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5631 - accuracy: 0.3386 - val_loss: 1.5862 - val_accuracy: 0.3371\n",
            "Epoch 69/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5669 - accuracy: 0.3381 - val_loss: 1.5403 - val_accuracy: 0.3461\n",
            "Epoch 70/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5407 - accuracy: 0.3385 - val_loss: 1.5371 - val_accuracy: 0.3456\n",
            "Epoch 71/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5440 - accuracy: 0.3412 - val_loss: 1.5486 - val_accuracy: 0.3429\n",
            "Epoch 72/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5586 - accuracy: 0.3381 - val_loss: 1.5859 - val_accuracy: 0.3352\n",
            "Epoch 73/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5591 - accuracy: 0.3381 - val_loss: 1.5392 - val_accuracy: 0.3451\n",
            "Epoch 74/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5531 - accuracy: 0.3363 - val_loss: 1.5442 - val_accuracy: 0.3388\n",
            "Epoch 75/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5409 - accuracy: 0.3392 - val_loss: 1.5412 - val_accuracy: 0.3430\n",
            "Epoch 76/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5396 - accuracy: 0.3424 - val_loss: 1.5463 - val_accuracy: 0.3391\n",
            "Epoch 77/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5411 - accuracy: 0.3415 - val_loss: 1.5527 - val_accuracy: 0.3444\n",
            "Epoch 78/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5520 - accuracy: 0.3421 - val_loss: 1.5527 - val_accuracy: 0.3414\n",
            "Epoch 79/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5485 - accuracy: 0.3408 - val_loss: 1.5548 - val_accuracy: 0.3409\n",
            "Epoch 80/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5526 - accuracy: 0.3381 - val_loss: 1.5587 - val_accuracy: 0.3313\n",
            "Epoch 81/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5370 - accuracy: 0.3389 - val_loss: 1.5398 - val_accuracy: 0.3431\n",
            "Epoch 82/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5392 - accuracy: 0.3420 - val_loss: 1.5390 - val_accuracy: 0.3444\n",
            "Epoch 83/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5424 - accuracy: 0.3424 - val_loss: 1.5826 - val_accuracy: 0.3301\n",
            "Epoch 84/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5536 - accuracy: 0.3384 - val_loss: 1.5887 - val_accuracy: 0.3359\n",
            "Epoch 85/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5608 - accuracy: 0.3359 - val_loss: 1.5564 - val_accuracy: 0.3408\n",
            "Epoch 86/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5457 - accuracy: 0.3408 - val_loss: 1.5492 - val_accuracy: 0.3413\n",
            "Epoch 87/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5382 - accuracy: 0.3429 - val_loss: 1.5392 - val_accuracy: 0.3440\n",
            "Epoch 88/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5394 - accuracy: 0.3417 - val_loss: 1.5469 - val_accuracy: 0.3441\n",
            "Epoch 89/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5408 - accuracy: 0.3399 - val_loss: 1.5475 - val_accuracy: 0.3433\n",
            "Epoch 90/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5413 - accuracy: 0.3402 - val_loss: 1.5373 - val_accuracy: 0.3452\n",
            "Epoch 91/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5597 - accuracy: 0.3375 - val_loss: 1.5854 - val_accuracy: 0.3364\n",
            "Epoch 92/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5661 - accuracy: 0.3379 - val_loss: 1.5509 - val_accuracy: 0.3433\n",
            "Epoch 93/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5627 - accuracy: 0.3395 - val_loss: 1.5611 - val_accuracy: 0.3414\n",
            "Epoch 94/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5464 - accuracy: 0.3408 - val_loss: 1.5553 - val_accuracy: 0.3423\n",
            "Epoch 95/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.5468 - accuracy: 0.3388 - val_loss: 1.5579 - val_accuracy: 0.3402\n",
            "Epoch 96/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5565 - accuracy: 0.3409 - val_loss: 1.5461 - val_accuracy: 0.3433\n",
            "Epoch 97/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5449 - accuracy: 0.3390 - val_loss: 1.5443 - val_accuracy: 0.3402\n",
            "Epoch 98/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5459 - accuracy: 0.3396 - val_loss: 1.5697 - val_accuracy: 0.3373\n",
            "Epoch 99/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5676 - accuracy: 0.3377 - val_loss: 1.5648 - val_accuracy: 0.3410\n",
            "Epoch 100/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5532 - accuracy: 0.3384 - val_loss: 1.5661 - val_accuracy: 0.3408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DteTcXPMaOF"
      },
      "source": [
        "### Our model is currently able to classify with an accuracy of 34%. Let's see if we can improve accuracy by normalising data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmEm88q0Myvy",
        "outputId": "3f916b82-04b4-4669-8d48-2d17e98502b7"
      },
      "source": [
        "# get the maximum and minimum values of training data\n",
        "train_data.min(), train_data.max()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 255)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNOPS6xtNCJN",
        "outputId": "da9620cc-8086-46ec-ea08-e7ef93600c28"
      },
      "source": [
        "# normalize data by dividing by max\n",
        "train_data_norm = train_data / 255\n",
        "test_data_norm = test_data / 255\n",
        "\n",
        "train_data_norm.min(), train_data_norm.max()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpNaCtCYNaJv",
        "outputId": "040a37cf-effe-4841-9ed0-67e548cead98"
      },
      "source": [
        "# create and train second model with normalised data\n",
        "# set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# create a model\n",
        "model_1 = tf.keras.Sequential()\n",
        "\n",
        "# add layers\n",
        "model_1.add(tf.keras.layers.Flatten(\n",
        "    input_shape=(28,28))),  # flatten data to linear vector\n",
        "model_1.add(tf.keras.layers.Dense(\n",
        "    units=4, activation=tf.keras.activations.relu))\n",
        "model_1.add(tf.keras.layers.Dense(\n",
        "    units=4, activation=tf.keras.activations.relu))\n",
        "model_1.add(tf.keras.layers.Dense(\n",
        "    units=10, activation=tf.keras.activations.softmax))\n",
        "\n",
        "# compile model\n",
        "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# train model on training data\n",
        "history_1 = model_1.fit(\n",
        "    train_data_norm,\n",
        "    tf.one_hot(train_labels, depth=10),\n",
        "    epochs=100,\n",
        "    validation_data=(test_data_norm, tf.one_hot(test_labels, depth=10))\n",
        ")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.0348 - accuracy: 0.6474 - val_loss: 0.6937 - val_accuracy: 0.7617\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.6376 - accuracy: 0.7757 - val_loss: 0.6400 - val_accuracy: 0.7820\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5942 - accuracy: 0.7914 - val_loss: 0.6247 - val_accuracy: 0.7783\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5750 - accuracy: 0.7979 - val_loss: 0.6078 - val_accuracy: 0.7881\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5641 - accuracy: 0.8006 - val_loss: 0.6169 - val_accuracy: 0.7881\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5544 - accuracy: 0.8043 - val_loss: 0.5855 - val_accuracy: 0.7951\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5488 - accuracy: 0.8063 - val_loss: 0.6097 - val_accuracy: 0.7836\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5428 - accuracy: 0.8077 - val_loss: 0.5787 - val_accuracy: 0.7971\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5373 - accuracy: 0.8097 - val_loss: 0.5698 - val_accuracy: 0.7977\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5360 - accuracy: 0.8124 - val_loss: 0.5658 - val_accuracy: 0.8014\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5311 - accuracy: 0.8130 - val_loss: 0.5714 - val_accuracy: 0.8002\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5284 - accuracy: 0.8132 - val_loss: 0.5626 - val_accuracy: 0.8027\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5271 - accuracy: 0.8138 - val_loss: 0.5619 - val_accuracy: 0.8041\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5249 - accuracy: 0.8143 - val_loss: 0.5718 - val_accuracy: 0.7991\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5231 - accuracy: 0.8148 - val_loss: 0.5706 - val_accuracy: 0.8024\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5203 - accuracy: 0.8162 - val_loss: 0.5731 - val_accuracy: 0.8023\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5191 - accuracy: 0.8176 - val_loss: 0.5594 - val_accuracy: 0.8030\n",
            "Epoch 18/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5176 - accuracy: 0.8157 - val_loss: 0.5582 - val_accuracy: 0.8053\n",
            "Epoch 19/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5156 - accuracy: 0.8169 - val_loss: 0.5644 - val_accuracy: 0.8007\n",
            "Epoch 20/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5146 - accuracy: 0.8177 - val_loss: 0.5660 - val_accuracy: 0.8075\n",
            "Epoch 21/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5125 - accuracy: 0.8197 - val_loss: 0.5684 - val_accuracy: 0.8004\n",
            "Epoch 22/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5112 - accuracy: 0.8180 - val_loss: 0.5666 - val_accuracy: 0.8029\n",
            "Epoch 23/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5105 - accuracy: 0.8195 - val_loss: 0.5570 - val_accuracy: 0.8068\n",
            "Epoch 24/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5101 - accuracy: 0.8177 - val_loss: 0.5565 - val_accuracy: 0.8076\n",
            "Epoch 25/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5075 - accuracy: 0.8196 - val_loss: 0.5572 - val_accuracy: 0.8048\n",
            "Epoch 26/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5081 - accuracy: 0.8193 - val_loss: 0.5659 - val_accuracy: 0.8065\n",
            "Epoch 27/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5058 - accuracy: 0.8206 - val_loss: 0.5587 - val_accuracy: 0.8045\n",
            "Epoch 28/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5052 - accuracy: 0.8206 - val_loss: 0.5561 - val_accuracy: 0.8077\n",
            "Epoch 29/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5050 - accuracy: 0.8205 - val_loss: 0.5664 - val_accuracy: 0.8054\n",
            "Epoch 30/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5024 - accuracy: 0.8213 - val_loss: 0.5575 - val_accuracy: 0.8051\n",
            "Epoch 31/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5029 - accuracy: 0.8215 - val_loss: 0.5627 - val_accuracy: 0.8033\n",
            "Epoch 32/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5012 - accuracy: 0.8204 - val_loss: 0.5574 - val_accuracy: 0.8097\n",
            "Epoch 33/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4998 - accuracy: 0.8219 - val_loss: 0.5774 - val_accuracy: 0.8007\n",
            "Epoch 34/100\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4996 - accuracy: 0.8222 - val_loss: 0.5596 - val_accuracy: 0.8073\n",
            "Epoch 35/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4980 - accuracy: 0.8237 - val_loss: 0.5507 - val_accuracy: 0.8095\n",
            "Epoch 36/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4959 - accuracy: 0.8245 - val_loss: 0.5515 - val_accuracy: 0.8066\n",
            "Epoch 37/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4961 - accuracy: 0.8236 - val_loss: 0.5614 - val_accuracy: 0.8000\n",
            "Epoch 38/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4948 - accuracy: 0.8248 - val_loss: 0.5453 - val_accuracy: 0.8111\n",
            "Epoch 39/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4942 - accuracy: 0.8242 - val_loss: 0.5580 - val_accuracy: 0.8044\n",
            "Epoch 40/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4926 - accuracy: 0.8248 - val_loss: 0.5527 - val_accuracy: 0.8057\n",
            "Epoch 41/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4923 - accuracy: 0.8246 - val_loss: 0.5585 - val_accuracy: 0.8025\n",
            "Epoch 42/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4920 - accuracy: 0.8244 - val_loss: 0.5437 - val_accuracy: 0.8104\n",
            "Epoch 43/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4912 - accuracy: 0.8250 - val_loss: 0.5520 - val_accuracy: 0.8081\n",
            "Epoch 44/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4905 - accuracy: 0.8268 - val_loss: 0.5465 - val_accuracy: 0.8090\n",
            "Epoch 45/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4899 - accuracy: 0.8261 - val_loss: 0.5440 - val_accuracy: 0.8113\n",
            "Epoch 46/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4887 - accuracy: 0.8257 - val_loss: 0.5522 - val_accuracy: 0.8076\n",
            "Epoch 47/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4874 - accuracy: 0.8263 - val_loss: 0.5491 - val_accuracy: 0.8100\n",
            "Epoch 48/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4874 - accuracy: 0.8267 - val_loss: 0.5517 - val_accuracy: 0.8092\n",
            "Epoch 49/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4873 - accuracy: 0.8260 - val_loss: 0.5568 - val_accuracy: 0.8102\n",
            "Epoch 50/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4865 - accuracy: 0.8274 - val_loss: 0.5441 - val_accuracy: 0.8130\n",
            "Epoch 51/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4852 - accuracy: 0.8279 - val_loss: 0.5471 - val_accuracy: 0.8084\n",
            "Epoch 52/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4850 - accuracy: 0.8284 - val_loss: 0.5508 - val_accuracy: 0.8066\n",
            "Epoch 53/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4847 - accuracy: 0.8276 - val_loss: 0.5539 - val_accuracy: 0.8047\n",
            "Epoch 54/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4844 - accuracy: 0.8299 - val_loss: 0.5495 - val_accuracy: 0.8084\n",
            "Epoch 55/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4837 - accuracy: 0.8293 - val_loss: 0.5450 - val_accuracy: 0.8104\n",
            "Epoch 56/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4830 - accuracy: 0.8280 - val_loss: 0.5404 - val_accuracy: 0.8125\n",
            "Epoch 57/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4823 - accuracy: 0.8268 - val_loss: 0.5537 - val_accuracy: 0.8110\n",
            "Epoch 58/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4832 - accuracy: 0.8279 - val_loss: 0.5476 - val_accuracy: 0.8042\n",
            "Epoch 59/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4807 - accuracy: 0.8288 - val_loss: 0.5539 - val_accuracy: 0.8131\n",
            "Epoch 60/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4815 - accuracy: 0.8284 - val_loss: 0.5436 - val_accuracy: 0.8104\n",
            "Epoch 61/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4812 - accuracy: 0.8298 - val_loss: 0.5495 - val_accuracy: 0.8084\n",
            "Epoch 62/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4811 - accuracy: 0.8292 - val_loss: 0.5415 - val_accuracy: 0.8125\n",
            "Epoch 63/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4808 - accuracy: 0.8287 - val_loss: 0.5490 - val_accuracy: 0.8056\n",
            "Epoch 64/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4793 - accuracy: 0.8285 - val_loss: 0.5442 - val_accuracy: 0.8097\n",
            "Epoch 65/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4780 - accuracy: 0.8304 - val_loss: 0.5423 - val_accuracy: 0.8123\n",
            "Epoch 66/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4796 - accuracy: 0.8294 - val_loss: 0.5525 - val_accuracy: 0.8119\n",
            "Epoch 67/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4789 - accuracy: 0.8303 - val_loss: 0.5444 - val_accuracy: 0.8131\n",
            "Epoch 68/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4778 - accuracy: 0.8298 - val_loss: 0.5521 - val_accuracy: 0.8090\n",
            "Epoch 69/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4773 - accuracy: 0.8290 - val_loss: 0.5454 - val_accuracy: 0.8124\n",
            "Epoch 70/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4783 - accuracy: 0.8291 - val_loss: 0.5468 - val_accuracy: 0.8125\n",
            "Epoch 71/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4773 - accuracy: 0.8305 - val_loss: 0.5444 - val_accuracy: 0.8091\n",
            "Epoch 72/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4764 - accuracy: 0.8304 - val_loss: 0.5570 - val_accuracy: 0.8094\n",
            "Epoch 73/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4760 - accuracy: 0.8306 - val_loss: 0.5453 - val_accuracy: 0.8106\n",
            "Epoch 74/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4763 - accuracy: 0.8307 - val_loss: 0.5557 - val_accuracy: 0.8123\n",
            "Epoch 75/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4760 - accuracy: 0.8305 - val_loss: 0.5420 - val_accuracy: 0.8097\n",
            "Epoch 76/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4756 - accuracy: 0.8301 - val_loss: 0.5594 - val_accuracy: 0.8082\n",
            "Epoch 77/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4774 - accuracy: 0.8293 - val_loss: 0.5425 - val_accuracy: 0.8098\n",
            "Epoch 78/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4753 - accuracy: 0.8306 - val_loss: 0.5428 - val_accuracy: 0.8144\n",
            "Epoch 79/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4747 - accuracy: 0.8316 - val_loss: 0.5503 - val_accuracy: 0.8056\n",
            "Epoch 80/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4744 - accuracy: 0.8308 - val_loss: 0.5481 - val_accuracy: 0.8090\n",
            "Epoch 81/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4742 - accuracy: 0.8322 - val_loss: 0.5484 - val_accuracy: 0.8072\n",
            "Epoch 82/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4746 - accuracy: 0.8310 - val_loss: 0.5483 - val_accuracy: 0.8116\n",
            "Epoch 83/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4740 - accuracy: 0.8321 - val_loss: 0.5717 - val_accuracy: 0.7986\n",
            "Epoch 84/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4739 - accuracy: 0.8319 - val_loss: 0.5466 - val_accuracy: 0.8087\n",
            "Epoch 85/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4742 - accuracy: 0.8310 - val_loss: 0.5502 - val_accuracy: 0.8097\n",
            "Epoch 86/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4737 - accuracy: 0.8319 - val_loss: 0.5444 - val_accuracy: 0.8121\n",
            "Epoch 87/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4735 - accuracy: 0.8317 - val_loss: 0.5401 - val_accuracy: 0.8111\n",
            "Epoch 88/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4731 - accuracy: 0.8325 - val_loss: 0.5475 - val_accuracy: 0.8116\n",
            "Epoch 89/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4730 - accuracy: 0.8314 - val_loss: 0.5473 - val_accuracy: 0.8069\n",
            "Epoch 90/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4718 - accuracy: 0.8325 - val_loss: 0.5477 - val_accuracy: 0.8104\n",
            "Epoch 91/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4725 - accuracy: 0.8323 - val_loss: 0.5446 - val_accuracy: 0.8145\n",
            "Epoch 92/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4719 - accuracy: 0.8334 - val_loss: 0.5413 - val_accuracy: 0.8158\n",
            "Epoch 93/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4719 - accuracy: 0.8316 - val_loss: 0.5514 - val_accuracy: 0.8101\n",
            "Epoch 94/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4710 - accuracy: 0.8328 - val_loss: 0.5481 - val_accuracy: 0.8142\n",
            "Epoch 95/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4703 - accuracy: 0.8328 - val_loss: 0.5401 - val_accuracy: 0.8157\n",
            "Epoch 96/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4704 - accuracy: 0.8335 - val_loss: 0.5433 - val_accuracy: 0.8126\n",
            "Epoch 97/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4702 - accuracy: 0.8333 - val_loss: 0.5411 - val_accuracy: 0.8174\n",
            "Epoch 98/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4699 - accuracy: 0.8340 - val_loss: 0.5478 - val_accuracy: 0.8110\n",
            "Epoch 99/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4702 - accuracy: 0.8335 - val_loss: 0.5459 - val_accuracy: 0.8119\n",
            "Epoch 100/100\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4692 - accuracy: 0.8345 - val_loss: 0.5425 - val_accuracy: 0.8142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2F6Gl1aUiNX"
      },
      "source": [
        "### Normalization of input data increases classification accuracy to 81%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8vlG7KUrGk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}